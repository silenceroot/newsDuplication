利用Simhash对相似新闻文本进行去重
马广通
	Simhash对于去重效果比较好，也就是两篇新闻文本有着大量的重复内容，比如说转载等方式，但对于语义相似的两篇文本效果不是很好。在文章长度比较长时效果要好，对于短文本不太适用。
	Simhash算法分为以下五个步骤：（1）对文本进行分词，得到文档的TF-IDF（2）对每个词频hash，得到每个词的hash值（3）根据每个词的权重进行hash加权（4）把文章的每个词频的加权的hash值进行相加合并（5）降维，根据相加的结果将一篇文档转化为64位二进制数来表示
	（1）在这一步骤中利用Python中的jieba分词模块来计算，首先import jieba.analyse,利用函数jieba.analyse.extract_tags(sentence, topK=20, withWeight=False, allowPOS=())得到tf-idf最大的那topK个词。对于topK的选择应该视文章大小来定，太大太小都会影响最终的结果。（有两个例子，第一个例子文章字数在1300字左右，topK=40；第二个例子文章字数在650个左右，topK=20or30）
	（2）可以使用Python自带的hash函数，但每次程序重新启动后相同的词的hash值不同，所以可以自己编写相关的函数。
	（3）根据hash值，如果为1，相应位置为权重的值，如果为0，相应位置为负的权重值
	（4）可以使用Python中的numpy库，每个词加权后的hash值为矩阵的一行，最后使用np.sum(array,axis=0)，进行合并
	（5）合并后的64-bit，相应位置大于0的置为1，否则置为0，得到文章的simhash值
	汉明距离：汉明距离指的是两个等长字符之间相应位置不同字符的个数，当两篇文章的simhash值小于3时，就认为两篇文章基本相似。在具体程序中将两个二进制数转化为整数（在字符串前面加上'0b'，再利用int（s,2）转化为整数），然后做异或运算，最后得到的整数转换为二进制数中1的个数就是汉明距离（参考剑指offer面试题10，速度比较快）。
	当面对数以万亿的数据时，我们应该如何快速的比较库中的相似文本。利用抽屉原理，把64-bit的simhash值分为四段，如果两个文本相似，那么必有一段是完全一样的。然后我们将库中的所有数据利用hashmap存储，在Python中个人认为可以用字典来存储，因为没有hashmap这种数据结构（java中有）。具体存储方式如下：对于一个64-bit的二进制数ID，分为四段ABCD，for i in (A,B,C,D):if i in dict.keys():dict[i].append(ID) else:dict[i]=[ID]。查询时，将待查询的数也分为四段，对于每一段在字典中找到对应的值集合，在集合中进行顺序比较，找到汉明距离小于3的数，进行四次查询后应该会有重复的结果，利用set去重就好。

